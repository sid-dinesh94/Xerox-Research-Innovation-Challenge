{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_age = pd.read_csv(\"id_age_train.csv\", sep=\",\")\n",
    "data_vitals = pd.read_csv(\"id_time_vitals_train.csv\", sep=\",\")\n",
    "data_labels = pd.read_csv(\"id_label_train.csv\", sep=\",\").set_index('ID')\n",
    "data_labs = pd.read_csv(\"id_time_labs_train.csv\", sep = \",\")\n",
    "data_timeseries = pd.merge(data_labs, data_vitals)\n",
    "patients = data_timeseries['ID']\n",
    "patient_ids = list(data_timeseries['ID'].unique())\n",
    "labels = np.zeros(len(patients))\n",
    "for p in patient_ids:\n",
    "    if int(data_labels.ix[p]['LABEL'])==1:\n",
    "        data_id = data_timeseries[data_timeseries['ID']==p]\n",
    "        indices = data_id[data_id['ICU']==1].index\n",
    "        num = len(indices)\n",
    "        for i in indices:\n",
    "            labels[i]=1\n",
    "data_timeseries['LABEL'] = labels\n",
    "data = pd.merge( data_age, data_timeseries, on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['V6'] = data['V6'].apply(lambda x: 80 if x<80 else 112 if x>112 else x)\n",
    "data['V5'] = data['V5'].apply(lambda x: 100 if x>100 else x if x>0 else np.nan )\n",
    "data['V4'] = data['V4'].apply(lambda x: x if x>0 else np.nan )\n",
    "data['V3'] = data['V3'].apply(lambda x: x if (x>30 and x<220) else np.nan )\n",
    "data['V2'] = data['V2'].apply(lambda x: x if (x>15 and x<200) else np.nan )\n",
    "data['V1'] = data['V1'].apply(lambda x: x if (x>30 and x<300) else np.nan )\n",
    "data['L1'] = data['L1'].apply(lambda x: x if (x>0 and x<14) else np.nan)\n",
    "data['L2'] = data['L2'].apply(lambda x: 132 if x>132 else x if x>0 else np.nan )\n",
    "data[['L3', 'L4', 'L5', 'L6', 'L16','L17', 'L13', 'L14', 'L21', 'L22', 'L24', 'L25']] = data[['L3', 'L4', 'L5', 'L6', 'L16','L17', 'L13', 'L14', 'L21', 'L22', 'L24', 'L25']].applymap(lambda x: x if x>0 else np.nan )\n",
    "data['L7'] = data['L7'].apply(lambda x: x if x<700 else 700 if x>700 else np.nan)\n",
    "data['L8'] = data['L8'].apply(lambda x: x if x<200 else 200 if x>200 else np.nan)\n",
    "data['L9'] = data['L9'].apply(lambda x: x if x<100 else x/1000)\n",
    "data['L10'] = data['L10'].apply(lambda x: x if (x>0 and x<100) else np.nan)\n",
    "data['L11'] = data['L11'].apply(lambda x: x if (x>0 and x<2000) else 2000 if x>2000 else np.nan)\n",
    "data['L12'] = data['L12'].apply(lambda x: x if x<5 else 5+(x-5)/10)\n",
    "data['L15'] =data['L15'].apply(lambda x: x if (x>0 and x<20) else 20 if x>20 else np.nan)\n",
    "data['L18'] =data['L18'].apply(lambda x: x if x<1000 else 1000 if x>1000 else np.nan)\n",
    "data['L19'] =data['L19'].apply(lambda x: x if x<800 else 800 if x>800 else np.nan)\n",
    "data['L20'] =data['L20'].apply(lambda x: x/100 if x>1000 else x if x>0 else np.nan)\n",
    "data['L23'] =data['L23'].apply(lambda x: 3000 if x>3000 else x if x>0 else -1*x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing\n",
      "...done\n"
     ]
    }
   ],
   "source": [
    "print \"Preprocessing\"\n",
    "data_grouped = data.groupby(['ID','TIME']).mean()\n",
    "data_nonicu = data[data['ICU']==0]\n",
    "data_icu = data[data['ICU']==1]\n",
    "normal = {attribute: data[attribute].mean() for attribute in data.columns}\n",
    "patient_ids = data['ID'].unique()\n",
    "for patient in patient_ids:\n",
    "    data_grouped.loc[patient, 0] = data_grouped.loc[patient, 0].fillna(normal)\n",
    "data_filled = data_grouped.reset_index()#.fillna(method='ffill')\n",
    "data_np = data_filled.as_matrix()\n",
    "print \"...done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_without_labels = data_filled[data_filled.columns[:-2]]\n",
    "data_by_id = data_without_labels.groupby('ID')\n",
    "timecolumns = data_filled.columns[3:-2]\n",
    "data_medians = data_by_id.apply(pd.expanding_median)[timecolumns].rename(columns=lambda x: 'MEDIAN_'+x)\n",
    "data_stds = data_by_id.apply(pd.expanding_std).fillna(0)[timecolumns].rename(columns=lambda x: 'STD_'+x)\n",
    "data_mins = data_by_id.apply(pd.expanding_min).fillna(0)[timecolumns].rename(columns=lambda x: 'MIN_'+x)\n",
    "data_maxs = data_by_id.apply(pd.expanding_max)[timecolumns].rename(columns=lambda x: 'MAX_'+x)\n",
    "data_counts = data_by_id.apply(pd.expanding_count)[timecolumns].rename(columns=lambda x: 'COUNT_'+x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_stats_full = pd.concat([data_without_labels[['ID','TIME','AGE']],data_medians, data_stds, data_mins, data_maxs, data_counts, data_filled[['ICU','LABEL']]], axis = 1)\n",
    "data_stats_np = data_stats_full.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%xdel data_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Preparing Validation Features\n",
    "Delete Cached dataframes before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_age = pd.read_csv(\"id_age_val.csv\", sep=\",\")\n",
    "data_vitals = pd.read_csv(\"id_time_vitals_val.csv\", sep=\",\")\n",
    "data_labels = pd.read_csv(\"id_label_val.csv\", sep=\",\", header = None, names = ['ID','LABEL']).set_index('ID')\n",
    "data_labs = pd.read_csv(\"id_time_labs_val.csv\", sep = \",\")\n",
    "data_timeseries = pd.merge(data_labs, data_vitals)\n",
    "patients = data_timeseries['ID']\n",
    "patient_ids = list(data_timeseries['ID'].unique())\n",
    "labels = np.zeros(len(patients))\n",
    "#for p in patient_ids:\n",
    "#    if int(data_labels.ix[p]['LABEL'])==1:\n",
    "#        data_id = data_timeseries[data_timeseries['ID']==p]\n",
    "#        indices = data_id[data_id['ICU']==1].index\n",
    "#        num = len(indices)\n",
    "#        for i in indices:\n",
    "#            labels[i]=1\n",
    "#data_timeseries['LABEL'] = labels\n",
    "data = pd.merge( data_age, data_timeseries, on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['V6'] = data['V6'].apply(lambda x: 80 if x<80 else 112 if x>112 else x)\n",
    "data['V5'] = data['V5'].apply(lambda x: 100 if x>100 else x if x>0 else np.nan )\n",
    "data['V4'] = data['V4'].apply(lambda x: x if x>0 else np.nan )\n",
    "data['V3'] = data['V3'].apply(lambda x: x if (x>30 and x<220) else np.nan )\n",
    "data['V2'] = data['V2'].apply(lambda x: x if (x>15 and x<200) else np.nan )\n",
    "data['V1'] = data['V1'].apply(lambda x: x if (x>30 and x<300) else np.nan )\n",
    "data['L1'] = data['L1'].apply(lambda x: x if (x>0 and x<14) else np.nan)\n",
    "data['L2'] = data['L2'].apply(lambda x: 132 if x>132 else x if x>0 else np.nan )\n",
    "#data[['L3', 'L4', 'L5', 'L6', 'L16','L17', 'L13', 'L14', 'L21', 'L22', 'L24', 'L25']] = data[['L3', 'L4', 'L5', 'L6', 'L16','L17', 'L13', 'L14', 'L21', 'L22', 'L24', 'L25']].applymap(lambda x: x if x>0 else np.nan )\n",
    "data['L7'] = data['L7'].apply(lambda x: x if x<700 else 700 if x>700 else np.nan)\n",
    "data['L8'] = data['L8'].apply(lambda x: x if x<200 else 200 if x>200 else np.nan)\n",
    "data['L9'] = data['L9'].apply(lambda x: x if x<100 else x/1000)\n",
    "data['L10'] = data['L10'].apply(lambda x: x if (x>0 and x<100) else np.nan)\n",
    "data['L11'] = data['L11'].apply(lambda x: x if (x>0 and x<2000) else 2000 if x>2000 else np.nan)\n",
    "data['L12'] = data['L12'].apply(lambda x: x if x<5 else 5+(x-5)/10)\n",
    "data['L15'] =data['L15'].apply(lambda x: x if (x>0 and x<20) else 20 if x>20 else np.nan)\n",
    "data['L18'] =data['L18'].apply(lambda x: x if x<1000 else 1000 if x>1000 else np.nan)\n",
    "data['L19'] =data['L19'].apply(lambda x: x if x<800 else 800 if x>800 else np.nan)\n",
    "data['L20'] =data['L20'].apply(lambda x: x/100 if x>1000 else x if x>0 else np.nan)\n",
    "data['L23'] =data['L23'].apply(lambda x: 3000 if x>3000 else x if x>0 else -1*x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_grouped = data.groupby(['ID','TIME']).mean()\n",
    "for patient in patient_ids:\n",
    "    data_grouped.loc[patient, 0] = data_grouped.loc[patient, 0].fillna(normal)\n",
    "data_filled = data_grouped.reset_index()#.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_by_id = data_filled.groupby('ID')\n",
    "timecolumns = data_filled.columns[3:-1]\n",
    "data_medians = data_by_id.apply(pd.expanding_median)[timecolumns].rename(columns=lambda x: 'MEDIAN_'+x)\n",
    "data_stds = data_by_id.apply(pd.expanding_std).fillna(0)[timecolumns].rename(columns=lambda x: 'STD_'+x)\n",
    "data_mins = data_by_id.apply(pd.expanding_min).fillna(0)[timecolumns].rename(columns=lambda x: 'MIN_'+x)\n",
    "data_maxs = data_by_id.apply(pd.expanding_max)[timecolumns].rename(columns=lambda x: 'MAX_'+x)\n",
    "data_counts = data_by_id.apply(pd.expanding_count)[timecolumns].rename(columns=lambda x: 'COUNT_'+x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_stats_full = pd.concat([data_filled[['ID','TIME','AGE']],data_medians, data_stds, data_mins, data_maxs, data_counts, data_filled['ICU']], axis = 1)\n",
    "val_stats_np = data_stats_full.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Preprocessed Validation set\n",
    "Deleted cached variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_feats = data_stats_np[:,2:-2]\n",
    "features= []\n",
    "for k in xrange(60,90,5):\n",
    "    kbest = SelectKBest(k=k)\n",
    "    kbest.fit(train_feats,data_stats_np[:,-1])\n",
    "    kfeatures= data_stats_full.columns[2:-2][kbest.get_support()]\n",
    "    kfeaturesfull = [['ID', 'TIME'],kfeatures,['ICU','LABEL']]\n",
    "    kfeaturesfullval = [['ID', 'TIME'], kfeatures,['ICU']]\n",
    "    kfeaturesfull = [item for sublist in kfeaturesfull for item in sublist]\n",
    "    kfeaturesfullval = [item for sublist in kfeaturesfullval for item in sublist]\n",
    "    features.append((k,kfeaturesfull,kfeaturesfullval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features[7][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import compute_class_weight\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "class_weights = compute_class_weight('auto', [0,1], data_stats_np[:,-1])\n",
    "class_weight_dictionary = {0:0.35, 1:0.65}\n",
    "class_weight_dictionary2 = {0:0.25, 1:0.75}\n",
    "class_weight_dictionary3 = {0:0.2, 1:0.8}\n",
    "class_weight_dictionary4 = {0: 0.3, 1:0.7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_labels = pd.read_csv(\"id_label_val.csv\", header = None, names = ['ID','LABEL'],sep=\",\").set_index('ID')\n",
    "#for p in data_stats_full['ID'].unique():\n",
    "#    if data_labels.ix[p]['LABEL']==1:\n",
    "#        data_stats_full = data_stats_full.append(data_stats_full[data_stats_full.ID==p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for triple in features[7:8]:\n",
    "    train_stats = data_stats_full[triple[1]].as_matrix()\n",
    "    val_stats = val_stats_full[triple[2]]\n",
    "    random = [41,np.random.randint(1,150),np.random.randint(1,150),np.random.randint(1,150)]\n",
    "    print random\n",
    "    clf1 = SGDClassifier(loss = 'squared_hinge',warm_start=True, random_state = random[0],penalty='elasticnet',n_iter=30, l1_ratio=0.20, class_weight=class_weight_dictionary3)\n",
    "    clf2 = SGDClassifier(loss = 'squared_hinge',warm_start=True, random_state = random[0],penalty='elasticnet',n_iter=30, l1_ratio=0.20, class_weight=class_weight_dictionary4)\n",
    "    #clf3 = SGDClassifier(loss = 'squared_hinge',warm_start=True, random_state = random[0],penalty='elasticnet',n_iter=30, l1_ratio=0.20, class_weight=class_weight_dictionary2)\n",
    "    clf3 = MultinomialNB(fit_prior = True, class_prior= [0.3,0.7])\n",
    "#    clf4 = MultinomialNB(fit_prior = True)\n",
    "    clf5 = SGDClassifier(loss = 'log',warm_start=True, random_state = random[0],penalty='elasticnet',n_iter=30, l1_ratio=0.20, class_weight=class_weight_dictionary)\n",
    "    \n",
    "    clfs = [clf1,clf2,clf5,clf4]\n",
    "    clf4.partial_fit(train_stats[:,2:-2],data_stats_full.as_matrix()[:,-1],[0,1])\n",
    "#        clf4.partial_fit(train_stats[:,2:-2],data_stats_np[:,-1],[0,1])\n",
    "\n",
    "    for clf in clfs[:-1]:\n",
    "        clf.fit(train_stats[:,2:-2],data_stats_full.as_matrix()[:,-1])\n",
    "    print 'Testing'\n",
    "    test_feats_grouped = val_stats.set_index('ID')\n",
    "    test_ids = test_feats_grouped.index.unique()\n",
    "    final_preds = [[],[],[],[],[]]\n",
    "    final_answers = []\n",
    "    for id in test_ids:\n",
    "        test_id = test_feats_grouped.ix[id]\n",
    "        test_id_np = test_id.as_matrix()\n",
    "        test_id_np = np.atleast_2d(test_id_np)\n",
    "        icu_indices = np.nonzero(test_id_np[:,-1])[0]\n",
    "        prev_prediction = 0\n",
    "        final_answers.append(data_labels.ix[int(id)]['LABEL'])\n",
    "        flag = [0,0,0,0,0]\n",
    "        final=[0,0,0,0,0]\n",
    "        icui=0\n",
    "        for ind in icu_indices:\n",
    "            partial_data_feats = test_id_np[:ind+1,1:-1]\n",
    "            if icui==0 and ind>0:\n",
    "                for clf in clfs:\n",
    "                    clf.partial_fit(partial_data_feats[:-1,:], np.zeros((partial_data_feats[:-1,:].shape[0],)))\n",
    "            if icui>0:\n",
    "                if ind - icu_indices[icui-1]>1 :\n",
    "                    for i in range(0,len(clfs)):\n",
    "                        results = np.empty((partial_data_feats[icu_indices[icui-1]:ind,:].shape[0],))\n",
    "                        results.fill(int(prev_prediction))\n",
    "                        clfs[i].partial_fit(partial_data_feats[icu_indices[icui-1]:ind,:], results)\n",
    "                else:\n",
    "                    for i in range(0, len(clfs)):\n",
    "                        prev_icu = np.atleast_2d(partial_data_feats[icu_indices[icui-1],:])\n",
    "                        clfs[i].partial_fit(prev_icu, prev_prediction)\n",
    "            for i in range(0,len(clfs)):\n",
    "                prediction = clfs[i].predict(partial_data_feats[-1,:])\n",
    "                prev_prediction = prediction\n",
    "                if flag[i]==0 and int(prediction[0])==1:\n",
    "                    final[i]=1\n",
    "                    flag[i] +=1\n",
    "            icui+=1\n",
    "        for i in range(0,len(clfs)):\n",
    "            final_preds[i].append(final[i])\n",
    "    \n",
    "        \n",
    "    print '...done'\n",
    "    print'Scoring'\n",
    "    for i in range(0,len(clfs)):\n",
    "        cm = confusion_matrix(final_answers, final_preds[i])\n",
    "        TN = cm[0][0]\n",
    "        FP = cm[0][1]\n",
    "        FN = cm[1][0]\n",
    "        TP = cm[1][1]\n",
    "        specificity = float(TN)/(TN+FP)\n",
    "        sensitivity = float(TP)/(TP+FN)\n",
    "        print 'K: ' + str(triple[0]) + '___spec=' + str(specificity) + '___sens= ' + str(sensitivity) + '___for clf' + str(i)\n",
    "    bagged_clf_preds = [1 if final_preds[1][x] + final_preds[2][x] + final_preds[0][x] + 2*final_preds[3][x]>2 else 0 for x in range(0,len(final_preds[0]))]\n",
    "    cm = confusion_matrix(final_answers,bagged_clf_preds)\n",
    "    TN = cm[0][0]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "    TP = cm[1][1]\n",
    "    specificity = float(TN)/(TN+FP)\n",
    "    sensitivity = float(TP)/(TP+FN)\n",
    "    print 'K: ' + str(triple[0]) + '___spec=' + str(specificity) + '___sens= ' + str(sensitivity) + '___for bagged clf'\n",
    "    print '...done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    clf1 = SGDClassifier(loss = 'squared_hinge',warm_start=True, random_state = random[0],penalty='elasticnet',n_iter=30, l1_ratio=0.20, class_weight=class_weight_dictionary3)\n",
    "    clf2 = SGDClassifier(loss = 'log',warm_start=True, random_state = random[0],penalty='elasticnet',n_iter=30, l1_ratio=0.20, class_weight=class_weight_dictionary2)\n",
    "    clf3 = SGDClassifier(loss = 'squared_hinge',warm_start=True, random_state = random[0],penalty='elasticnet',n_iter=30, l1_ratio=0.20, class_weight=class_weight_dictionary)\n",
    "    \n",
    "[11, 129, 85]\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 30___spec=0.999103942652___sens= 0.0365853658537___for clf0\n",
    "K: 30___spec=1.0___sens= 0.0___for clf1\n",
    "K: 30___spec=0.996415770609___sens= 0.0609756097561___for clf2\n",
    "K: 30___spec=0.999103942652___sens= 0.0365853658537___for bagged clf\n",
    "...done\n",
    "[7, 109, 124]\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 35___spec=0.996415770609___sens= 0.0487804878049___for clf0\n",
    "K: 35___spec=1.0___sens= 0.0243902439024___for clf1\n",
    "K: 35___spec=0.979390681004___sens= 0.146341463415___for clf2\n",
    "K: 35___spec=0.996415770609___sens= 0.0609756097561___for bagged clf\n",
    "...done\n",
    "[58, 17, 85]\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 40___spec=0.998207885305___sens= 0.0365853658537___for clf0\n",
    "K: 40___spec=1.0___sens= 0.0___for clf1\n",
    "K: 40___spec=0.994623655914___sens= 0.19512195122___for clf2\n",
    "K: 40___spec=0.999103942652___sens= 0.0365853658537___for bagged clf\n",
    "...done\n",
    "[89, 113, 18]\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 45___spec=0.994623655914___sens= 0.158536585366___for clf0\n",
    "K: 45___spec=1.0___sens= 0.0487804878049___for clf1\n",
    "K: 45___spec=0.990143369176___sens= 0.182926829268___for clf2\n",
    "K: 45___spec=0.996415770609___sens= 0.134146341463___for bagged clf\n",
    "...done\n",
    "[34, 58, 27]\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 50___spec=0.998207885305___sens= 0.0365853658537___for clf0\n",
    "K: 50___spec=1.0___sens= 0.0121951219512___for clf1\n",
    "K: 50___spec=0.991039426523___sens= 0.0975609756098___for clf2\n",
    "K: 50___spec=0.998207885305___sens= 0.0365853658537___for bagged clf\n",
    "...done\n",
    "[128, 148, 117]\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 55___spec=0.996415770609___sens= 0.0487804878049___for clf0\n",
    "K: 55___spec=0.999103942652___sens= 0.0243902439024___for clf1\n",
    "K: 55___spec=0.989247311828___sens= 0.121951219512___for clf2\n",
    "K: 55___spec=0.997311827957___sens= 0.0487804878049___for bagged clf\n",
    "...done\n",
    "[82, 109, 54]\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 60___spec=0.993727598566___sens= 0.0731707317073___for clf0\n",
    "K: 60___spec=1.0___sens= 0.0___for clf1\n",
    "K: 60___spec=0.991039426523___sens= 0.19512195122___for clf2\n",
    "K: 60___spec=0.998207885305___sens= 0.0609756097561___for bagged clf\n",
    "...done\n",
    "[41, 91, 145]\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 65___spec=0.992831541219___sens= 0.182926829268___for clf0\n",
    "K: 65___spec=1.0___sens= 0.0365853658537___for clf1\n",
    "K: 65___spec=0.987455197133___sens= 0.219512195122___for clf2\n",
    "K: 65___spec=0.995519713262___sens= 0.170731707317___for bagged clf\n",
    "...done\n",
    "[69, 41, 50]\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 70___spec=0.999103942652___sens= 0.0487804878049___for clf0\n",
    "K: 70___spec=0.997311827957___sens= 0.0609756097561___for clf1\n",
    "K: 70___spec=0.977598566308___sens= 0.19512195122___for clf2\n",
    "K: 70___spec=0.997311827957___sens= 0.0487804878049___for bagged clf\n",
    "...done\n",
    "[96, 124, 61]\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 75___spec=0.991935483871___sens= 0.109756097561___for clf0\n",
    "K: 75___spec=0.991935483871___sens= 0.109756097561___for clf1\n",
    "K: 75___spec=0.990143369176___sens= 0.19512195122___for clf2\n",
    "K: 75___spec=0.992831541219___sens= 0.109756097561___for bagged clf\n",
    "...done\n",
    "[10, 23, 57]\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 80___spec=0.981182795699___sens= 0.170731707317___for clf0\n",
    "K: 80___spec=0.995519713262___sens= 0.0853658536585___for clf1\n",
    "K: 80___spec=0.974014336918___sens= 0.19512195122___for clf2\n",
    "K: 80___spec=0.98476702509___sens= 0.134146341463___for bagged clf\n",
    "...done\n",
    "[139, 103, 85]\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 85___spec=0.995519713262___sens= 0.109756097561___for clf0\n",
    "K: 85___spec=0.995519713262___sens= 0.0853658536585___for clf1\n",
    "K: 85___spec=0.987455197133___sens= 0.182926829268___for clf2\n",
    "K: 85___spec=0.995519713262___sens= 0.121951219512___for bagged clf\n",
    "...done\n",
    "[16, 79, 81]\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 90___spec=0.991039426523___sens= 0.170731707317___for clf0\n",
    "K: 90___spec=0.986559139785___sens= 0.146341463415___for clf1\n",
    "K: 90___spec=0.976702508961___sens= 0.243902439024___for clf2\n",
    "K: 90___spec=0.987455197133___sens= 0.170731707317___for bagged clf\n",
    "...done\n",
    "[28, 54, 114]\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 95___spec=0.994623655914___sens= 0.134146341463___for clf0\n",
    "K: 95___spec=0.998207885305___sens= 0.0853658536585___for clf1\n",
    "K: 95___spec=0.986559139785___sens= 0.182926829268___for clf2\n",
    "K: 95___spec=0.996415770609___sens= 0.109756097561___for bagged clf\n",
    "...done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    clf = SGDClassifier(loss = 'log',warm_start=True, penalty='elasticnet',n_iter=30)\n",
    "\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 30___spec=0.996415770609___sens= 0.0731707317073\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 35___spec=0.996415770609___sens= 0.0487804878049\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 40___spec=0.991935483871___sens= 0.0609756097561\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 45___spec=0.991935483871___sens= 0.0975609756098\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 50___spec=0.997311827957___sens= 0.146341463415\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 55___spec=0.995519713262___sens= 0.0731707317073\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 60___spec=0.997311827957___sens= 0.134146341463\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 65___spec=0.998207885305___sens= 0.0487804878049\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 70___spec=0.992831541219___sens= 0.0975609756098\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 75___spec=0.986559139785___sens= 0.170731707317\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 80___spec=0.974910394265___sens= 0.30487804878\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 85___spec=0.990143369176___sens= 0.121951219512\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 90___spec=0.974014336918___sens= 0.207317073171\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 95___spec=0.982974910394___sens= 0.170731707317\n",
    "...done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    clf = SGDClassifier(loss = 'log',warm_start=True, penalty='elasticnet',n_iter=30, l1_ratio=0.2)\n",
    "\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 30___spec=0.999103942652___sens= 0.0243902439024\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 35___spec=0.993727598566___sens= 0.0853658536585\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 40___spec=0.999103942652___sens= 0.121951219512\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 45___spec=0.996415770609___sens= 0.0487804878049\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 50___spec=0.995519713262___sens= 0.0487804878049\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 55___spec=1.0___sens= 0.0487804878049\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 60___spec=0.998207885305___sens= 0.0609756097561\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 65___spec=0.999103942652___sens= 0.0365853658537\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 70___spec=0.998207885305___sens= 0.121951219512\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 75___spec=0.997311827957___sens= 0.121951219512\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 80___spec=0.982078853047___sens= 0.170731707317\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 85___spec=0.994623655914___sens= 0.109756097561\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 90___spec=0.997311827957___sens= 0.0487804878049\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 95___spec=0.992831541219___sens= 0.121951219512\n",
    "...done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    clf = SGDClassifier(loss = 'log',warm_start=True, penalty='elasticnet',n_iter=30, l1_ratio=0.25)\n",
    "\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 30___spec=1.0___sens= 0.0121951219512\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 35___spec=0.999103942652___sens= 0.0487804878049\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 40___spec=1.0___sens= 0.0609756097561\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 45___spec=1.0___sens= 0.0\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 50___spec=0.991935483871___sens= 0.0731707317073\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 55___spec=0.993727598566___sens= 0.0731707317073\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 60___spec=0.994623655914___sens= 0.0975609756098\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 65___spec=0.997311827957___sens= 0.0487804878049\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 70___spec=0.999103942652___sens= 0.0365853658537\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 75___spec=0.993727598566___sens= 0.0853658536585\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 80___spec=0.987455197133___sens= 0.121951219512\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 85___spec=0.98835125448___sens= 0.146341463415\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 90___spec=0.975806451613___sens= 0.268292682927\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 95___spec=0.986559139785___sens= 0.121951219512\n",
    "...done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    clf = SGDClassifier(loss = 'hinge_squared',warm_start=True, penalty='elasticnet',n_iter=30, l1_ratio=0.25)\n",
    "\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 30___spec=0.993727598566___sens= 0.0731707317073\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 35___spec=0.998207885305___sens= 0.0365853658537\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 40___spec=0.999103942652___sens= 0.0365853658537\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 45___spec=0.999103942652___sens= 0.0975609756098\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 50___spec=0.987455197133___sens= 0.134146341463\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 55___spec=0.991039426523___sens= 0.0975609756098\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 60___spec=0.998207885305___sens= 0.0609756097561\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 65___spec=0.997311827957___sens= 0.0609756097561\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 70___spec=0.996415770609___sens= 0.0731707317073\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 75___spec=0.998207885305___sens= 0.146341463415\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 80___spec=0.974910394265___sens= 0.207317073171\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 85___spec=0.990143369176___sens= 0.121951219512\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 90___spec=0.996415770609___sens= 0.0975609756098\n",
    "Testing\n",
    "...done\n",
    "Scoring\n",
    "K: 95___spec=0.985663082437___sens= 0.219512195122\n",
    "...done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testvitals = 'id_time_vitals_test.csv'\n",
    "testlabs = 'id_time_labs_test.csv'\n",
    "testage = 'id_age_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = read_test(testvitals,testlabs,testage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data_p = preprocess(test_data,normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess(data, normal):\n",
    "    data['V6'] = data['V6'].apply(lambda x: 80 if x<80 else 112 if x>112 else x)\n",
    "    data['V5'] = data['V5'].apply(lambda x: 100 if x>100 else x if x>0 else np.nan )\n",
    "    data['V4'] = data['V4'].apply(lambda x: x if x>0 else np.nan )\n",
    "    data['V3'] = data['V3'].apply(lambda x: x if (x>30 and x<220) else np.nan )\n",
    "    data['V2'] = data['V2'].apply(lambda x: x if (x>15 and x<200) else np.nan )\n",
    "    data['V1'] = data['V1'].apply(lambda x: x if (x>30 and x<300) else np.nan )\n",
    "    data['L1'] = data['L1'].apply(lambda x: x if (x>0 and x<14) else np.nan)\n",
    "    data['L2'] = data['L2'].apply(lambda x: 132 if x>132 else x if x>0 else np.nan )\n",
    "    data[['L3', 'L4', 'L5', 'L6', 'L16','L17', 'L13', 'L14', 'L21', 'L22', 'L24', 'L25']] = data[['L3', 'L4', 'L5', 'L6', 'L16','L17', 'L13', 'L14', 'L21', 'L22', 'L24', 'L25']].applymap(lambda x: x if x>0 else np.nan )\n",
    "    data['L7'] = data['L7'].apply(lambda x: x if x<700 else 700 if x>700 else np.nan)\n",
    "    data['L8'] = data['L8'].apply(lambda x: x if x<200 else 200 if x>200 else np.nan)\n",
    "    data['L9'] = data['L9'].apply(lambda x: x if x<100 else x/1000)\n",
    "    data['L10'] = data['L10'].apply(lambda x: x if (x>0 and x<100) else np.nan)\n",
    "    data['L11'] = data['L11'].apply(lambda x: x if (x>0 and x<2000) else 2000 if x>2000 else np.nan)\n",
    "    data['L12'] = data['L12'].apply(lambda x: x if x<5 else 5+(x-5)/10)\n",
    "    data['L15'] =data['L15'].apply(lambda x: x if (x>0 and x<20) else 20 if x>20 else np.nan)\n",
    "    data['L18'] =data['L18'].apply(lambda x: x if x<1000 else 1000 if x>1000 else np.nan)\n",
    "    data['L19'] =data['L19'].apply(lambda x: x if x<800 else 800 if x>800 else np.nan)\n",
    "    data['L20'] =data['L20'].apply(lambda x: x/100 if x>1000 else x if x>0 else np.nan)\n",
    "    data['L23'] =data['L23'].apply(lambda x: 3000 if x>3000 else x if x>0 else -1*x)\n",
    "    data_grouped = data.groupby(['ID','TIME']).mean()\n",
    "    patient_ids = data['ID'].unique()\n",
    "    for patient in patient_ids:\n",
    "        data_grouped.loc[patient, 0] = data_grouped.loc[patient, 0].fillna(normal)\n",
    "    data_p = data_grouped.reset_index()\n",
    "    return data_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_test(testvitals, testlabs, testage):\n",
    "    data_age_test = pd.read_csv(testage, sep=\",\")\n",
    "    data_vitals_test = pd.read_csv(testvitals, sep=\",\")\n",
    "    data_labs_test = pd.read_csv(testlabs, sep = \",\")\n",
    "    data_timeseries_test = pd.merge(data_labs_test, data_vitals_test)\n",
    "    #patient_ids_test = list(data_timeseries_test['ID'].unique())\n",
    "    data_test = pd.merge( data_age_test, data_timeseries_test, on='ID')\n",
    "    return data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_feat_filled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features[7][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_feat_filled = make_features_test(test_data_p,features[7][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_features_test(data_p,k_features):\n",
    "    data_without_labels = data_p[data_p.columns[:-1]]\n",
    "    data_by_id =  data_without_labels.groupby('ID')\n",
    "    timecolumns = data_p.columns[3:-1]\n",
    "    data_medians = data_by_id.apply(pd.expanding_median)[timecolumns].rename(columns=lambda x: 'MEDIAN_'+x)\n",
    "    data_stds = data_by_id.apply(pd.expanding_std).fillna(0)[timecolumns].rename(columns=lambda x: 'STD_'+x)\n",
    "    data_mins = data_by_id.apply(pd.expanding_min)[timecolumns].rename(columns=lambda x: 'MIN_'+x)\n",
    "    data_maxs = data_by_id.apply(pd.expanding_max)[timecolumns].rename(columns=lambda x: 'MAX_'+x)\n",
    "    data_counts = data_by_id.apply(pd.expanding_count)[timecolumns].rename(columns=lambda x: 'COUNT_'+x)\n",
    "    data_with_stats = pd.concat([data_medians, data_stds, data_mins, data_maxs, data_counts], axis = 1)\n",
    "    data_subset_full = pd.concat([data_p[['ID','TIME','AGE']], data_with_stats,data_p[['ICU']]], axis=1)\n",
    "    data_subset = data_subset_full[k_features]\n",
    "    \n",
    "    return data_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_test_preds = predict(test_feat_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('output.csv', 'w') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    for pred in final_test_preds:\n",
    "        writer.writerow([pred[0], pred[1], pred[2]])\n",
    "print \"...done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_labels = pd.read_csv('output.csv',header = None, names = ['ID','TIME','LABEL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped_labels = test_labels.groupby('ID').sum().reset_index()\n",
    "len(grouped_labels[grouped_labels.LABEL>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(data_test):\n",
    "    train_stats = data_stats_full[triple[1]].as_matrix()\n",
    "    val_stats = val_stats_full[triple[2]]\n",
    "    random = [41,np.random.randint(1,150),np.random.randint(1,150),np.random.randint(1,150)]\n",
    "    print random\n",
    "    clf1 = SGDClassifier(loss = 'squared_hinge',warm_start=True, random_state = random[0],penalty='elasticnet',n_iter=30, l1_ratio=0.20, class_weight=class_weight_dictionary3)\n",
    "    clf2 = SGDClassifier(loss = 'squared_hinge',warm_start=True, random_state = random[0],penalty='elasticnet',n_iter=30, l1_ratio=0.20, class_weight=class_weight_dictionary4)\n",
    "    clf3 = SGDClassifier(loss = 'log',warm_start=True, random_state = random[0],penalty='elasticnet',n_iter=30, l1_ratio=0.20, class_weight=class_weight_dictionary)\n",
    "    clf4 = MultinomialNB(fit_prior = True, class_prior= [0.3,0.7])\n",
    "    clfs = [clf1,clf2,clf3,clf4]\n",
    "    clf4.partial_fit(train_stats[:,2:-2],data_stats_full.as_matrix()[:,-1],[0,1])\n",
    "    for clf in clfs[:-1]:\n",
    "        clf.fit(train_stats[:,2:-2],data_stats_full.as_matrix()[:,-1])\n",
    "    test_feats_grouped = data_test.set_index('ID')\n",
    "    test_ids = test_feats_grouped.index.unique()\n",
    "    final_predictions = []\n",
    "    for id in test_ids:\n",
    "        test_id = test_feats_grouped.ix[id]\n",
    "        test_id_np = np.atleast_2d(test_id.as_matrix())\n",
    "        icu_indices = np.nonzero(test_id_np[:,-1])[0]\n",
    "        prev_prediction = [0,0,0,0]\n",
    "        icui=0\n",
    "        for ind in icu_indices:\n",
    "            partial_data_feats = test_id_np[:ind+1,1:-1]\n",
    "            if icui==0 and ind>0:\n",
    "                for clf in clfs:\n",
    "                    clf.partial_fit(partial_data_feats[:-1,:], np.zeros((partial_data_feats[:-1,:].shape[0],)))\n",
    "            if icui>0:\n",
    "                for i in range(0,len(clfs)): \n",
    "                    if ind - icu_indices[icui-1]>1:\n",
    "                        results = np.empty((partial_data_feats[icu_indices[icui-1]:ind,:].shape[0],))\n",
    "                        results.fill(int(prev_prediction[i]))\n",
    "                        clfs[i].partial_fit(partial_data_feats[icu_indices[icui-1]:ind,:], results)\n",
    "                    else:\n",
    "                        prev_icu = np.atleast_2d(partial_data_feats[icu_indices[icui-1],:])\n",
    "                        clfs[i].partial_fit(prev_icu, np.atleast_1d(prev_prediction[i]))\n",
    "            bagged_prediction = 0\n",
    "            for i in range(0,len(clfs)-1):\n",
    "                prediction = int(clfs[i].predict(partial_data_feats[-1,:])[0])\n",
    "                prev_prediction[i] = prediction\n",
    "                bagged_prediction += prediction\n",
    "            prediction = int(clf4.predict(partial_data_feats[-1,:])[0])\n",
    "            prev_prediction[3]=prediction\n",
    "            bagged_prediction += 2*prediction\n",
    "            if bagged_prediction >2:\n",
    "                bagged_final_prediction = 1\n",
    "            else:\n",
    "                bagged_final_prediction = 0\n",
    "            final_predictions.append((int(id), int(test_id_np[ind, 0]), int(bagged_final_prediction)))\n",
    "            icui+=1\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
